{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13041499,"sourceType":"datasetVersion","datasetId":8258245}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\npip install -q transformers datasets accelerate evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport math\nimport pandas as pd\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\nfrom accelerate import Accelerator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T17:21:13.822769Z","iopub.execute_input":"2025-09-13T17:21:13.822971Z","iopub.status.idle":"2025-09-13T17:21:24.867121Z","shell.execute_reply.started":"2025-09-13T17:21:13.822946Z","shell.execute_reply":"2025-09-13T17:21:24.866551Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T17:21:24.868635Z","iopub.execute_input":"2025-09-13T17:21:24.869076Z","iopub.status.idle":"2025-09-13T17:21:24.872706Z","shell.execute_reply.started":"2025-09-13T17:21:24.869057Z","shell.execute_reply":"2025-09-13T17:21:24.871977Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"CSV_PATH = \"instruction_following_dataset.csv\"   # your CSV\nMODEL_NAME = \"microsoft/deberta-v3-base\"\nOUTPUT_DIR = \"judge1_reward_model_alt\"\nMAX_LENGTH = 512\nBATCH_SIZE = 8             # reduce if OOM\nNUM_EPOCHS = 3\nLEARNING_RATE = 2e-5\nWARMUP_STEPS = 500         # tune\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\naccelerator = Accelerator(\n    gradient_accumulation_steps=4,  # accumulate instead of large batch\n    mixed_precision=\"fp16\"\n)\n\nDEVICE = accelerator.device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T17:21:24.873382Z","iopub.execute_input":"2025-09-13T17:21:24.873603Z","iopub.status.idle":"2025-09-13T17:21:24.977852Z","shell.execute_reply.started":"2025-09-13T17:21:24.873578Z","shell.execute_reply":"2025-09-13T17:21:24.977154Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/instruction-following-dataset/instruction_following_dataset.csv').sample(n=100000, random_state=42)\nprint(df.shape)\nhf_ds = Dataset.from_pandas(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T17:23:39.403472Z","iopub.execute_input":"2025-09-13T17:23:39.404152Z","iopub.status.idle":"2025-09-13T17:23:41.630140Z","shell.execute_reply.started":"2025-09-13T17:23:39.404117Z","shell.execute_reply":"2025-09-13T17:23:41.629573Z"}},"outputs":[{"name":"stdout","text":"(100000, 3)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def build_pair_texts(examples):\n    prompts = examples[\"prompt\"]\n    chosen = examples[\"chosen_response\"]\n    rejected = examples[\"rejected_response\"]\n    chosen_texts = [f\"Question: {p}\\n\\nAnswer: {c}\" for p,c in zip(prompts, chosen)]\n    rejected_texts = [f\"Question: {p}\\n\\nAnswer: {r}\" for p,r in zip(prompts, rejected)]\n    return {\"chosen_text\": chosen_texts, \"rejected_text\": rejected_texts}\n\nprint(\"Creating pairwise texts...\")\nhf_ds = hf_ds.map(build_pair_texts, batched=True, remove_columns=[c for c in hf_ds.column_names if c not in []])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T17:23:51.378118Z","iopub.execute_input":"2025-09-13T17:23:51.378395Z","iopub.status.idle":"2025-09-13T17:23:52.386027Z","shell.execute_reply.started":"2025-09-13T17:23:51.378376Z","shell.execute_reply":"2025-09-13T17:23:52.385268Z"}},"outputs":[{"name":"stdout","text":"Creating pairwise texts...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b39b853235d243b792ff2d8828de6b66"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T17:23:52.387499Z","iopub.execute_input":"2025-09-13T17:23:52.387770Z","iopub.status.idle":"2025-09-13T17:24:16.435793Z","shell.execute_reply.started":"2025-09-13T17:23:52.387749Z","shell.execute_reply":"2025-09-13T17:24:16.435140Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38b508b0d6cc4fdea43546d3b8ba12ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e77f3c29d6e84c71a76199aa67774280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58065ebf4e3e44579bf5707a9c567b7d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n2025-09-13 17:24:02.528541: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757784242.709718      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757784242.771049      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46d2200a7e764aaf9f98df99a004cbdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d5ea35cdc2247c8a6d18855bef2f9c8"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class PairwiseTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_dataset):\n        self.ds = hf_dataset\n    def __len__(self):\n        return len(self.ds)\n    def __getitem__(self, idx):\n        item = self.ds[int(idx)]\n        return {\"chosen_text\": item[\"chosen_text\"], \"rejected_text\": item[\"rejected_text\"]}\n\ndef collate_fn(batch):\n    chosen_texts = [b[\"chosen_text\"] for b in batch]\n    rejected_texts = [b[\"rejected_text\"] for b in batch]\n    chosen_enc = tokenizer(chosen_texts, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n    rejected_enc = tokenizer(rejected_texts, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n    return {\n        \"chosen_input_ids\": chosen_enc[\"input_ids\"],\n        \"chosen_attention_mask\": chosen_enc[\"attention_mask\"],\n        \"rejected_input_ids\": rejected_enc[\"input_ids\"],\n        \"rejected_attention_mask\": rejected_enc[\"attention_mask\"],\n    }\n\ntorch_ds = PairwiseTorchDataset(hf_ds)\ndataloader = DataLoader(\n    torch_ds, batch_size=BATCH_SIZE, shuffle=True,\n    collate_fn=collate_fn, num_workers=0, pin_memory=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T17:24:16.436825Z","iopub.execute_input":"2025-09-13T17:24:16.437335Z","iopub.status.idle":"2025-09-13T17:24:16.443731Z","shell.execute_reply.started":"2025-09-13T17:24:16.437316Z","shell.execute_reply":"2025-09-13T17:24:16.443189Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\ntotal_steps = len(dataloader) * NUM_EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=min(WARMUP_STEPS, total_steps),\n    num_training_steps=total_steps\n)\n\nmodel, optimizer, dataloader, scheduler = accelerator.prepare(\n    model, optimizer, dataloader, scheduler\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T17:24:16.444484Z","iopub.execute_input":"2025-09-13T17:24:16.444714Z","iopub.status.idle":"2025-09-13T17:24:16.840937Z","shell.execute_reply.started":"2025-09-13T17:24:16.444693Z","shell.execute_reply":"2025-09-13T17:24:16.840189Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"global_step = 0\nmodel.train()\n\nfor epoch in range(NUM_EPOCHS):\n    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", disable=not accelerator.is_main_process)\n    for step, batch in enumerate(pbar):\n        optimizer.zero_grad()\n\n        chosen_inputs = {\n            \"input_ids\": batch[\"chosen_input_ids\"],\n            \"attention_mask\": batch[\"chosen_attention_mask\"],\n        }\n        rejected_inputs = {\n            \"input_ids\": batch[\"rejected_input_ids\"],\n            \"attention_mask\": batch[\"rejected_attention_mask\"],\n        }\n\n        with accelerator.autocast():\n            logits_chosen = model(**chosen_inputs).logits.squeeze(-1)     # (B,)\n            logits_rejected = model(**rejected_inputs).logits.squeeze(-1) # (B,)\n            diff = logits_chosen - logits_rejected\n            loss = -(torch.log(torch.sigmoid(diff) + 1e-12)).mean()\n\n        accelerator.backward(loss)\n        optimizer.step()\n        scheduler.step()\n\n        global_step += 1\n        if accelerator.is_main_process and step % 50 == 0:\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"step\": global_step})\n\n    # ---- Save checkpoint at end of epoch ----\n    if accelerator.is_main_process:\n        save_path = os.path.join(OUTPUT_DIR, f\"checkpoint_epoch_{epoch+1}\")\n        accelerator.unwrap_model(model).save_pretrained(save_path)\n        tokenizer.save_pretrained(save_path)\n        accelerator.save_state(save_path)\n        print(f\"Checkpoint saved to {save_path}\")\n\n# -----------------------------\n# FINAL SAVE + ZIP FOR DOWNLOAD\n# -----------------------------\nif accelerator.is_main_process:\n    final_model_path = os.path.join(OUTPUT_DIR, \"final_model\")\n    accelerator.unwrap_model(model).save_pretrained(final_model_path)\n    tokenizer.save_pretrained(final_model_path)\n    accelerator.save_state(final_model_path)\n    print(f\"Final model saved to {final_model_path}\")\n\n    # Zip the model for easy download (Kaggle/Colab)\n    shutil.make_archive(\"trained_model\", 'zip', final_model_path)\n    print(\"Zipped model to trained_model.zip\")\n\nprint(\"âœ… Training complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T17:24:16.842363Z","iopub.execute_input":"2025-09-13T17:24:16.842629Z","iopub.status.idle":"2025-09-13T22:06:57.691596Z","shell.execute_reply.started":"2025-09-13T17:24:16.842607Z","shell.execute_reply":"2025-09-13T22:06:57.689833Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1/3:   0%|          | 0/12500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14c627d99634491ca493dc34ef313a04"}},"metadata":{}},{"name":"stdout","text":"Checkpoint saved to judge1_reward_model_alt/checkpoint_epoch_1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/3:   0%|          | 0/12500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a93f335e0fa740a59c31de0c7da8b97e"}},"metadata":{}},{"name":"stdout","text":"Checkpoint saved to judge1_reward_model_alt/checkpoint_epoch_2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/3:   0%|          | 0/12500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3883ddf6c6054043bd57eab56ce6e120"}},"metadata":{}},{"name":"stdout","text":"Checkpoint saved to judge1_reward_model_alt/checkpoint_epoch_3\nFinal model saved to judge1_reward_model_alt/final_model\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2611150128.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Zip the model for easy download (Kaggle/Colab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trained_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Zipped model to trained_model.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'shutil' is not defined"],"ename":"NameError","evalue":"name 'shutil' is not defined","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"import shutil\n\n# Choose final model or last checkpoint\nto_zip = final_model_path if os.path.exists(final_model_path) else latest_checkpoint\nshutil.make_archive(\"trained_model\", 'zip', to_zip)\nprint(\"Zipped model to trained_model.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T22:08:53.261468Z","iopub.execute_input":"2025-09-13T22:08:53.261755Z","iopub.status.idle":"2025-09-13T22:10:38.129606Z","shell.execute_reply.started":"2025-09-13T22:08:53.261734Z","shell.execute_reply":"2025-09-13T22:10:38.128947Z"}},"outputs":[{"name":"stdout","text":"Zipped model to trained_model.zip\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\nprint(\"Saving model to\", OUTPUT_DIR)\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(\"Done.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T22:11:01.323504Z","iopub.execute_input":"2025-09-13T22:11:01.324068Z","iopub.status.idle":"2025-09-13T22:11:03.816704Z","shell.execute_reply.started":"2025-09-13T22:11:01.324044Z","shell.execute_reply":"2025-09-13T22:11:03.815833Z"}},"outputs":[{"name":"stdout","text":"Saving model to judge1_reward_model_alt\nDone.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"OUTPUT_DIR =\"/kaggle/working/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T22:06:57.695117Z","iopub.status.idle":"2025-09-13T22:06:57.695440Z","shell.execute_reply.started":"2025-09-13T22:06:57.695282Z","shell.execute_reply":"2025-09-13T22:06:57.695298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}