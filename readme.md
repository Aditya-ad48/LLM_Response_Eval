# âš¡ EvalDash â€“ LLM Evaluation Analytics  
[![Build Passing](https://img.shields.io/badge/build-passing-brightgreen.svg)](https://github.com/yourorg/yourrepo) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![Made with Flask/React](https://img.shields.io/badge/Made%20with-Flask%20%26%20React-blue.svg)](https://github.com)

EvalDash is a **full-stack platform** for evaluating and analyzing Large Language Model (LLM) responses.  
It combines a **modern React (TypeScript) dashboard frontend** with a **Flask (Python) backend**, enabling **multi-judge automated evaluation** (instruction following, hallucination detection, coherence scoring) on bulk LLM outputs.

---

## ğŸ“ Abstract  

EvalDash provides a **robust, extensible framework** for automated evaluation of LLM responses.  
It integrates **traditional NLP models** (DistilBERT, DeBERTa) with **modern APIs** (Gemini) and **web search** to deliver **multi-dimensional quality metrics**.  
The **interactive dashboard** lets researchers and practitioners **upload, analyze, and compare** LLM outputs at scale.

---

## âœ¨ Key Features  

- ğŸ” **Automated Multi-Judge Evaluation** â€“ Instruction following, hallucination detection, and coherence scoring.  
- ğŸ“Š **Interactive Dashboard** â€“ Visualize results, filter by agent/model, prompt, or metric.  
- ğŸ“‚ **Bulk File Upload** â€“ Supports **JSON, CSV, and DOCX** formats.  
- ğŸŒ **RESTful API** â€“ Simple integration with other tools and pipelines.  
- ğŸ¨ **Modern UI** â€“ Responsive, themeable, and mobile-friendly.  
- ğŸ”— **CORS Ready** â€“ Works seamlessly across local or remote setups.

## ğŸ—ï¸ Architecture

**Frontend (React/TypeScript):**  
Communicates with the backend via HTTP/JSON.

**Backend (Flask/Python):**  
Handles API requests and orchestrates evaluations.

**Judge System:**  
- **DistilBERT:** Instruction evaluation  
- **DeBERTa:** NLI and hallucination detection  
- **Gemini API:** Coherence scoring  
- **Web Search (DDGS):** Supplementary data gathering

## ğŸš€ Getting Started  

### âœ… Prerequisites  

- **Backend:** Python `3.8+`, pip, HuggingFace account, Gemini API key  
- **Frontend:** Node.js `16+`, npm or yarn

---

### ğŸ“¥ Installation  

#### 1ï¸âƒ£ Clone the Repository  
```bash
git clone <repository-url>
cd E6
```

#### 2ï¸âƒ£ Backend Setup

```bash
cd backend
python -m venv myenv
source myenv/bin/activate   # On Windows: myenv\Scripts\activate

pip install flask flask-cors flask-session python-dotenv
pip install transformers sentence-transformers torch
pip install google-generativeai huggingface-hub
pip install sentencepiece tiktoken ddgs
pip install pandas tqdm docx2txt
```

Create `.env` file:

```bash
cat > .env << EOF
HF_TOKEN=your_huggingface_token_here
GEMINI_API_KEY=your_gemini_api_key_here
CLIENT_URI=http://localhost:8080
EOF
```

#### 3ï¸âƒ£ Frontend Setup

```bash
cd ../frontend
npm install
```

Create `.env` file:

```bash
cat > .env << EOF
VITE_BACKEND_URL=http://127.0.0.1:5000/api
EOF
```

---

## ğŸ¯ Usage

### â–¶ Start Backend

```bash
cd backend
source myenv/bin/activate
python app.py
```

ğŸ‘‰ Runs at **[http://127.0.0.1:5000](http://127.0.0.1:5000)**

### â–¶ Start Frontend

```bash
cd frontend
npm run dev
```

ğŸ‘‰ Runs at **[http://localhost:8080](http://localhost:8080)**

---

### ğŸ“¤ Upload Data

Prepare a file (`JSON/CSV/DOCX`) in this format:

```json
[
  {
    "prompt_id": "p001",
    "prompt": "Write a short poem about technology",
    "response": "Digital dreams flow through silicon streams...",
    "agent_id": "gpt-4"
  }
]
```

Go to the dashboard â†’ **Upload File** â†’ **View Results** âœ…

---

## ğŸ“š API Documentation

### ğŸ”¹ POST `/api/evaluate/`

**Request**

```json
[
  {
    "prompt_id": "string",
    "prompt": "string",
    "response": "string",
    "agent_id": "string"
  }
]
```

**Response**

```json
[
  {
    "prompt_id": "string",
    "prompt": "string",
    "response": "string",
    "agent_id": "string",
    "instruction_score": 0.8542,
    "coherence_score": 3.7234,
    "is_hallucination": false,
    "nli_prediction": "entailment"
  }
]
```

---

## ğŸ§ª Evaluation Judges

* ğŸ§­ **Judge 1: Instruction Following** â†’ DistilBERT classifier (score: `0â€“1`).
* ğŸš¨ **Judge 2: Hallucination Detection** â†’ DeBERTa NLI + Cross-Encoder + Web Search â†’ returns **boolean** + **NLI label**.
* ğŸ”— **Judge 3: Coherence Scoring** â†’ Local transformer + Gemini API (score: `1â€“5`).

---

## ğŸ“ Project Structure

```
E6/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ controllers/
â”‚   â”œâ”€â”€ judges/
â”‚   â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â””â”€â”€ README.md
```

---

## âš™ï¸ Configuration

### ğŸ”¹ Backend `.env`

* `HF_TOKEN` â†’ HuggingFace token  
* `GEMINI_API_KEY` â†’ Gemini API key  
* `CLIENT_URI` â†’ Frontend URL  

### ğŸ”¹ Frontend `.env`

* `VITE_BACKEND_URL` â†’ Backend API base URL

---

## ğŸ†˜ Troubleshooting

* âŒ **Judge 2 not loading** â†’ `pip install sentencepiece tiktoken`
* ğŸŒ **CORS errors** â†’ Ensure `CLIENT_URI` matches frontend URL.
* ğŸ”‘ **Model access/auth** â†’ Check `HF_TOKEN` validity.
* ğŸ’¾ **Memory issues** â†’ Use smaller models for testing.

---

## ğŸ¤ Contributing

1. Fork the repo ğŸ´  
2. Create a **feature branch** ğŸŒ±  
3. Commit & push ğŸ”¼  
4. Open a **Pull Request** ğŸ“¬

---

## ğŸ“ License

Licensed under **MIT License** â€“ see `LICENSE`.

Built with â¤ï¸ for the **LLM evaluation community** ğŸš€
